train_result1 = as.matrix(t(pred(nn1, nntrain6[,1:12])), nrow = length(nntrain6[,1]), ncol = 2)
test_result1 = as.matrix(t(pred(nn1, nntest6[,1:12])), nrow = length(nntest6[,1]), ncol= 2)
table(train_result1[,1],  nntrain6$BK)
table(train_result1[,2],  nntrain6$Not_BK)
table(test_result1[,1],  nntest6$BK)
table(test_result1[,2],  nntest6$Not_BK)
mean(test_result1 != nntest6[,c("BK","Not_BK")])
mean(train_result1 != nntrain6[,c("BK","Not_BK")])
clean_polish_dt = read.table("cleandata.txt" , header = TRUE)
pca = prcomp(clean_polish_dt[,-c(58,57,56,47,46,27,26,21,14)])
X = as.matrix(clean_polish_dt[,-c(58,57,56,47,46,27,26,21,14)])
M = pca$rotation[,1:5]
pca_polish_data = as.data.frame((X %*% M))
pca_polish_data = log((pca_polish_data^2))
pca_polish_data = cbind(pca_polish_data,clean_polish_dt[,c(58,57,56,47,46,27,26,21,14)])
pca_polish_data$year = as.factor(pca_polish_data$year)
data_sp = pca_polish_data%>%group_by(year) # group them by year / we are using the cleaned file after imputation !!
# install.packages("caTools")
require(caTools)
data_sp$rownumber = 1:nrow(data_sp) # an indicator column for each row to check if the split worked fine
set.seed(123)
sample = sample.split(data_sp, SplitRatio = 2/3) # we pick 2/3 split ratio
train = subset(data_sp, sample == TRUE)%>%as.data.frame() # 1/3 proportion of the training set
test = subset(data_sp, sample == FALSE)%>%as.data.frame() # 2/3 proportion for the test set
# the split worked fine so if we want
# we can delete the extra column
train = subset(train , select = -rownumber)
test = subset(test , select = -rownumber)
train = train[as.numeric(train$year) <= 2,]
test = test[as.numeric(test$year) <= 2,]
train2 = train
test2 = test
train = data.frame(train[,c(1:6,8:14)])
test = data.frame(test[,c(1:6,8:14)])
train[,14] = train2[,7]
test[,14] = test2[,7]
library(e1071)
library(ISLR)
require(class)
library(sparsediscrim)
xnames = colnames(train2)
xnames = xnames[c(1:6,8:14)]
nntrain = data.frame(train2[c(1:6,8:14)])
nntest = data.frame(test2[c(1:6,8:14)])
nntrain$class = train2[,7]
nntest$class = test2[,7]
# have to break down V58 into 2 columns
nntrain2 = data.frame(model.matrix(~factor(nntrain$class)-1))
nntest2 = data.frame(model.matrix(~factor(nntest$class)-1))
#get rid of year for now:
nntrain = nntrain[,-6]
nntest = nntest[,-6]
#Now see if we can center the data:
nntrain4 = scale(data.frame(nntrain[,1:12]))
nntest4 = scale(data.frame(nntest[,1:12]))
nntrain5 = data.frame(nntrain4)
nntest5 = data.frame(nntest4)
# Add back the two columns of BK and non-BK
nntrain6 = cbind(nntrain5, nntrain2)
nntest6 = cbind(nntest5, nntest2)
colnames(nntrain6)[13] = "Not_BK"
colnames(nntrain6)[14] = "BK"
colnames(nntest6)[13] = "Not_BK"
colnames(nntest6)[14] = "BK"
# Now we do NNEts
require(neuralnet)
n = names(nntrain6[,1:12])
f= as.formula(paste("BK + Not_BK ~", paste(n, collapse = " + ")))
# Does this work?
nn1 = neuralnet(f, data= nntrain6, hidden= c(10, 8), linear.output = F, lifesign = "full", lifesign.step = 100)
clean_polish_dt = read.table("cleandata.txt" , header = TRUE)
pca = prcomp(clean_polish_dt[,-c(58,57,56,47,46,27,26,21,14)])
X = as.matrix(clean_polish_dt[,-c(58,57,56,47,46,27,26,21,14)])
M = pca$rotation[,1:5]
pca_polish_data = as.data.frame((X %*% M))
pca_polish_data = log((pca_polish_data^2))
pca_polish_data = cbind(pca_polish_data,clean_polish_dt[,c(58,57,56,47,46,27,26,21,14)])
pca_polish_data$year = as.factor(pca_polish_data$year)
data_sp = pca_polish_data%>%group_by(year) # group them by year / we are using the cleaned file after imputation !!
# install.packages("caTools")
require(caTools)
data_sp$rownumber = 1:nrow(data_sp) # an indicator column for each row to check if the split worked fine
set.seed(123)
sample = sample.split(data_sp, SplitRatio = 2/3) # we pick 2/3 split ratio
train = subset(data_sp, sample == TRUE)%>%as.data.frame() # 1/3 proportion of the training set
test = subset(data_sp, sample == FALSE)%>%as.data.frame() # 2/3 proportion for the test set
# the split worked fine so if we want
# we can delete the extra column
train = subset(train , select = -rownumber)
test = subset(test , select = -rownumber)
train = train[as.numeric(train$year) == 2,]
test = test[as.numeric(test$year) == 2,]
train2 = train
test2 = test
train = data.frame(train[,c(1:6,8:14)])
test = data.frame(test[,c(1:6,8:14)])
train[,14] = train2[,7]
test[,14] = test2[,7]
library(e1071)
library(ISLR)
require(class)
library(sparsediscrim)
xnames = colnames(train2)
xnames = xnames[c(1:6,8:14)]
nntrain = data.frame(train2[c(1:6,8:14)])
nntest = data.frame(test2[c(1:6,8:14)])
nntrain$class = train2[,7]
nntest$class = test2[,7]
# have to break down V58 into 2 columns
nntrain2 = data.frame(model.matrix(~factor(nntrain$class)-1))
nntest2 = data.frame(model.matrix(~factor(nntest$class)-1))
#get rid of year for now:
nntrain = nntrain[,-6]
nntest = nntest[,-6]
#Now see if we can center the data:
nntrain4 = scale(data.frame(nntrain[,1:12]))
nntest4 = scale(data.frame(nntest[,1:12]))
nntrain5 = data.frame(nntrain4)
nntest5 = data.frame(nntest4)
# Add back the two columns of BK and non-BK
nntrain6 = cbind(nntrain5, nntrain2)
nntest6 = cbind(nntest5, nntest2)
colnames(nntrain6)[13] = "Not_BK"
colnames(nntrain6)[14] = "BK"
colnames(nntest6)[13] = "Not_BK"
colnames(nntest6)[14] = "BK"
# Now we do NNEts
require(neuralnet)
n = names(nntrain6[,1:12])
f= as.formula(paste("BK + Not_BK ~", paste(n, collapse = " + ")))
# Does this work?
nn1 = neuralnet(f, data= nntrain6, hidden= c(10, 8), linear.output = F, lifesign = "full", lifesign.step = 100)
# Let's do NNets
clean_polish_dt = read.table("cleandata.txt" , header = TRUE)
pca = prcomp(clean_polish_dt[,-c(58,57,56,47,46,27,26,21,14)])
X = as.matrix(clean_polish_dt[,-c(58,57,56,47,46,27,26,21,14)])
M = pca$rotation[,1:5]
pca_polish_data = as.data.frame((X %*% M))
pca_polish_data = log((pca_polish_data^2))
pca_polish_data = cbind(pca_polish_data,clean_polish_dt[,c(58,57,56,47,46,27,26,21,14)])
pca_polish_data$year = as.factor(pca_polish_data$year)
data_sp = pca_polish_data%>%group_by(year) # group them by year / we are using the cleaned file after imputation !!
# install.packages("caTools")
require(caTools)
data_sp$rownumber = 1:nrow(data_sp) # an indicator column for each row to check if the split worked fine
set.seed(123)
sample = sample.split(data_sp, SplitRatio = 2/3) # we pick 2/3 split ratio
train = subset(data_sp, sample == TRUE)%>%as.data.frame() # 1/3 proportion of the training set
test = subset(data_sp, sample == FALSE)%>%as.data.frame() # 2/3 proportion for the test set
# the split worked fine so if we want
# we can delete the extra column
train = subset(train , select = -rownumber)
test = subset(test , select = -rownumber)
train = train[as.numeric(train$year) == 5,]
test = test[as.numeric(test$year) == 5,]
train2 = train
test2 = test
train = data.frame(train[,c(1:6,8:14)])
test = data.frame(test[,c(1:6,8:14)])
train[,14] = train2[,7]
test[,14] = test2[,7]
library(e1071)
library(ISLR)
require(class)
library(sparsediscrim)
xnames = colnames(train2)
xnames = xnames[c(1:6,8:14)]
nntrain = data.frame(train2[c(1:6,8:14)])
nntest = data.frame(test2[c(1:6,8:14)])
nntrain$class = train2[,7]
nntest$class = test2[,7]
# have to break down V58 into 2 columns
nntrain2 = data.frame(model.matrix(~factor(nntrain$class)-1))
nntest2 = data.frame(model.matrix(~factor(nntest$class)-1))
#get rid of year for now:
nntrain = nntrain[,-6]
nntest = nntest[,-6]
#Now see if we can center the data:
nntrain4 = scale(data.frame(nntrain[,1:12]))
nntest4 = scale(data.frame(nntest[,1:12]))
nntrain5 = data.frame(nntrain4)
nntest5 = data.frame(nntest4)
# Add back the two columns of BK and non-BK
nntrain6 = cbind(nntrain5, nntrain2)
nntest6 = cbind(nntest5, nntest2)
colnames(nntrain6)[13] = "Not_BK"
colnames(nntrain6)[14] = "BK"
colnames(nntest6)[13] = "Not_BK"
colnames(nntest6)[14] = "BK"
# Now we do NNEts
require(neuralnet)
n = names(nntrain6[,1:12])
f= as.formula(paste("BK + Not_BK ~", paste(n, collapse = " + ")))
# Does this work?
nn1 = neuralnet(f, data= nntrain6, hidden= c(10, 8), linear.output = F, lifesign = "full", lifesign.step = 100)
pred = function(nn, dat) {
yhat = compute(nn, dat)$net.result
yhat = apply(yhat, 1, round) # rounding is better
return(yhat)
}
train_result1 = as.matrix(t(pred(nn1, nntrain6[,1:12])), nrow = length(nntrain6[,1]), ncol = 2)
test_result1 = as.matrix(t(pred(nn1, nntest6[,1:12])), nrow = length(nntest6[,1]), ncol= 2)
# Test results of Spam NNet
table(train_result1[,1],  nntrain6$BK)
table(train_result1[,2],  nntrain6$Not_BK)
table(test_result1[,1],  nntest6$BK)
table(test_result1[,2],  nntest6$Not_BK)
mean(test_result1 != nntest6[,c("BK","Not_BK")])
mean(train_result1 != nntrain6[,c("BK","Not_BK")])
clean_polish_dt = read.table("cleandata.txt" , header = TRUE)
pca = prcomp(clean_polish_dt[,-c(58,57,56,47,46,27,26,21,14)])
X = as.matrix(clean_polish_dt[,-c(58,57,56,47,46,27,26,21,14)])
M = pca$rotation[,1:5]
pca_polish_data = as.data.frame((X %*% M))
pca_polish_data = log((pca_polish_data^2))
pca_polish_data = cbind(pca_polish_data,clean_polish_dt[,c(58,57,56,47,46,27,26,21,14)])
pca_polish_data$year = as.factor(pca_polish_data$year)
data_sp = pca_polish_data%>%group_by(year) # group them by year / we are using the cleaned file after imputation !!
# install.packages("caTools")
require(caTools)
data_sp$rownumber = 1:nrow(data_sp) # an indicator column for each row to check if the split worked fine
set.seed(123)
sample = sample.split(data_sp, SplitRatio = 2/3) # we pick 2/3 split ratio
train = subset(data_sp, sample == TRUE)%>%as.data.frame() # 1/3 proportion of the training set
test = subset(data_sp, sample == FALSE)%>%as.data.frame() # 2/3 proportion for the test set
# the split worked fine so if we want
# we can delete the extra column
train = subset(train , select = -rownumber)
test = subset(test , select = -rownumber)
#train = train[as.numeric(train$year) == 5,]
#test = test[as.numeric(test$year) == 5,]
train2 = train
test2 = test
train = data.frame(train[,c(1:6,8:14)])
test = data.frame(test[,c(1:6,8:14)])
train[,14] = train2[,7]
test[,14] = test2[,7]
library(e1071)
library(ISLR)
require(class)
library(sparsediscrim)
xnames = colnames(train2)
xnames = xnames[c(1:6,8:14)]
nntrain = data.frame(train2[c(1:6,8:14)])
nntest = data.frame(test2[c(1:6,8:14)])
nntrain$class = train2[,7]
nntest$class = test2[,7]
# have to break down V58 into 2 columns
nntrain2 = data.frame(model.matrix(~factor(nntrain$class)-1))
nntest2 = data.frame(model.matrix(~factor(nntest$class)-1))
#get rid of year for now:
nntrain = nntrain[,-6]
nntest = nntest[,-6]
#Now see if we can center the data:
nntrain4 = scale(data.frame(nntrain[,1:12]))
nntest4 = scale(data.frame(nntest[,1:12]))
nntrain5 = data.frame(nntrain4)
nntest5 = data.frame(nntest4)
# Add back the two columns of BK and non-BK
nntrain6 = cbind(nntrain5, nntrain2)
nntest6 = cbind(nntest5, nntest2)
colnames(nntrain6)[13] = "Not_BK"
colnames(nntrain6)[14] = "BK"
colnames(nntest6)[13] = "Not_BK"
colnames(nntest6)[14] = "BK"
# Now we do NNEts
require(neuralnet)
n = names(nntrain6[,1:12])
f= as.formula(paste("BK + Not_BK ~", paste(n, collapse = " + ")))
# Does this work?
nn1 = neuralnet(f, data= nntrain6, hidden= c(10, 8), threshold = 0.1, linear.output = F, lifesign = "full", lifesign.step = 100)
pred = function(nn, dat) {
yhat = compute(nn, dat)$net.result
yhat = apply(yhat, 1, round) # rounding is better
return(yhat)
}
train_result1 = as.matrix(t(pred(nn1, nntrain6[,1:12])), nrow = length(nntrain6[,1]), ncol = 2)
test_result1 = as.matrix(t(pred(nn1, nntest6[,1:12])), nrow = length(nntest6[,1]), ncol= 2)
# Test results of Spam NNet
table(train_result1[,1],  nntrain6$BK)
table(train_result1[,2],  nntrain6$Not_BK)
table(test_result1[,1],  nntest6$BK)
table(test_result1[,2],  nntest6$Not_BK)
mean(test_result1 != nntest6[,c("BK","Not_BK")])
mean(train_result1 != nntrain6[,c("BK","Not_BK")])
nn2 = neuralnet(f, data= nntrain6, hidden= 12, threshold = 0.1, linear.output = F, lifesign = "full", lifesign.step = 100)
train_result2 = as.matrix(t(pred(nn2, nntrain6[,1:12])), nrow = length(nntrain6[,1]), ncol = 2)
test_result2 = as.matrix(t(pred(nn2, nntest6[,1:12])), nrow = length(nntest6[,1]), ncol= 2)
# Test results of Spam NNet
table(train_result2[,1],  nntrain6$BK)
table(train_result2[,2],  nntrain6$Not_BK)
table(test_result2[,1],  nntest6$BK)
table(test_result2[,2],  nntest6$Not_BK)
mean(test_result2 != nntest6[,c("BK","Not_BK")])
mean(train_result2 != nntrain6[,c("BK","Not_BK")])
nn3 = neuralnet(f, data= nntrain6, hidden= 20, threshold = 0.1, linear.output = F, lifesign = "full", lifesign.step = 100)
train_result3 = as.matrix(t(pred(nn3, nntrain6[,1:12])), nrow = length(nntrain6[,1]), ncol = 2)
test_result3 = as.matrix(t(pred(nn3, nntest6[,1:12])), nrow = length(nntest6[,1]), ncol= 2)
table(train_result3[,1],  nntrain6$BK)
table(train_result3[,2],  nntrain6$Not_BK)
table(test_result3[,1],  nntest6$BK)
table(test_result3[,2],  nntest6$Not_BK)
mean(test_result3 != nntest6[,c("BK","Not_BK")])
mean(train_result3 != nntrain6[,c("BK","Not_BK")])
Mean_test_error = rep(0, 14)
Mean_train_error = rep(0,14)
NN = list()
NN[1] = 1
NN[[1]] = 1
NN[[1][2]] = 1
NN[[1]][[1]] = 1
NN
NN = list()
clean_polish_dt = read.table("cleandata.txt" , header = TRUE)
pca = prcomp(clean_polish_dt[,-c(58,57,56,47,46,27,26,21,14)])
X = as.matrix(clean_polish_dt[,-c(58,57,56,47,46,27,26,21,14)])
M = pca$rotation[,1:5]
pca_polish_data = as.data.frame((X %*% M))
pca_polish_data = log((pca_polish_data^2))
pca_polish_data = cbind(pca_polish_data,clean_polish_dt[,c(58,57,56,47,46,27,26,21,14)])
pca_polish_data$year = as.factor(pca_polish_data$year)
data_sp = pca_polish_data%>%group_by(year) # group them by year / we are using the cleaned file after imputation !!
# install.packages("caTools")
require(caTools)
data_sp$rownumber = 1:nrow(data_sp) # an indicator column for each row to check if the split worked fine
set.seed(123)
sample = sample.split(data_sp, SplitRatio = 2/3) # we pick 2/3 split ratio
train = subset(data_sp, sample == TRUE)%>%as.data.frame() # 1/3 proportion of the training set
test = subset(data_sp, sample == FALSE)%>%as.data.frame() # 2/3 proportion for the test set
# the split worked fine so if we want
# we can delete the extra column
train = subset(train , select = -rownumber)
test = subset(test , select = -rownumber)
#train = train[as.numeric(train$year) == 5,]
#test = test[as.numeric(test$year) == 5,]
train2 = train
test2 = test
train = data.frame(train[,c(1:6,8:14)])
test = data.frame(test[,c(1:6,8:14)])
train[,14] = train2[,7]
test[,14] = test2[,7]
library(e1071)
library(ISLR)
require(class)
library(sparsediscrim)
xnames = colnames(train2)
xnames = xnames[c(1:6,8:14)]
nntrain = data.frame(train2[c(1:6,8:14)])
nntest = data.frame(test2[c(1:6,8:14)])
nntrain$class = train2[,7]
nntest$class = test2[,7]
# have to break down V58 into 2 columns
nntrain2 = data.frame(model.matrix(~factor(nntrain$class)-1))
nntest2 = data.frame(model.matrix(~factor(nntest$class)-1))
#get rid of year for now:
nntrain = nntrain[,-6]
nntest = nntest[,-6]
#Now see if we can center the data:
nntrain4 = scale(data.frame(nntrain[,1:12]))
nntest4 = scale(data.frame(nntest[,1:12]))
nntrain5 = data.frame(nntrain4)
nntest5 = data.frame(nntest4)
# Add back the two columns of BK and non-BK
nntrain6 = cbind(nntrain5, nntrain2)
nntest6 = cbind(nntest5, nntest2)
colnames(nntrain6)[13] = "Not_BK"
colnames(nntrain6)[14] = "BK"
colnames(nntest6)[13] = "Not_BK"
colnames(nntest6)[14] = "BK"
# Now we do NNEts
require(neuralnet)
n = names(nntrain6[,1:12])
f= as.formula(paste("BK + Not_BK ~", paste(n, collapse = " + ")))
pred = function(nn, dat) {
yhat = compute(nn, dat)$net.result
yhat = apply(yhat, 1, round) # rounding is better
return(yhat)
}
NN = list()
train_result = list()
test_result  = list()
Mean_test_error = rep(0, 14)
Mean_train_error = rep(0,14)
table_test = list()
table_train = list()
for(i in 2:3){
NN[[i]] = neuralnet(f, data= nntrain6, hidden= i, threshold = 0.1, linear.output = F, lifesign = "full", lifesign.step = 100)
train_result[[i]] = as.matrix(t(pred(NN[[i]], nntrain6[,1:12])), nrow = length(nntrain6[,1]), ncol = 2)
test_result[[i]] = as.matrix(t(pred(NN[[i]], nntest6[,1:12])), nrow = length(nntest6[,1]), ncol= 2)
Mean_test_error[[i]] = mean(test_result[[i]] != nntest6[,c("BK","Not_BK")])
Mean_train_error[[i]] = mean(train_result[[i]] != nntrain6[,c("BK","Not_BK")])
table_test[[i]] = table(test_result[[i]][,1],  nntrain6$BK)
table_train[[i]] = table(train_result[[i]][,1],  nntrain6$BK)
}
for(i in 2:3){
NN[[i]] = neuralnet(f, data= nntrain6, hidden= i, threshold = 0.1, linear.output = F, lifesign = "full", lifesign.step = 100)
train_result[[i]] = as.matrix(t(pred(NN[[i]], nntrain6[,1:12])), nrow = length(nntrain6[,1]), ncol = 2)
test_result[[i]] = as.matrix(t(pred(NN[[i]], nntest6[,1:12])), nrow = length(nntest6[,1]), ncol= 2)
Mean_test_error[[i]] = mean(test_result[[i]] != nntest6[,c("BK","Not_BK")])
Mean_train_error[[i]] = mean(train_result[[i]] != nntrain6[,c("BK","Not_BK")])
table_test[i] = table(test_result[i][,1],  nntrain6$BK)
table_train[i] = table(train_result[i][,1],  nntrain6$BK)
}
test_result[1,1]
test_result[[1,1]]
test_result[[1]][[1]]
test_result[[1]][[2]]
test_result[[2]][[2]]
for(i in 2:3){
NN[[i]] = neuralnet(f, data= nntrain6, hidden= i, threshold = 0.1, linear.output = F, lifesign = "full", lifesign.step = 100)
train_result[[i]] = as.matrix(t(pred(NN[[i]], nntrain6[,1:12])), nrow = length(nntrain6[,1]), ncol = 2)
test_result[[i]] = as.matrix(t(pred(NN[[i]], nntest6[,1:12])), nrow = length(nntest6[,1]), ncol= 2)
Mean_test_error[[i]] = mean(test_result[[i]] != nntest6[,c("BK","Not_BK")])
Mean_train_error[[i]] = mean(train_result[[i]] != nntrain6[,c("BK","Not_BK")])
table_test[i] = table(test_result[[i]][[,1]],  nntrain6$BK)
table_train[i] = table(train_result[[i]][[,1]],  nntrain6$BK)
}
test_result[[1]][[1]]
test_result[[1]][[2]]
test_result[[2]][[2]]
test_result[[2]][[,2]]
for(i in 2:3){
NN[[i]] = neuralnet(f, data= nntrain6, hidden= i, threshold = 0.1, linear.output = F, lifesign = "full", lifesign.step = 100)
train_result[[i]] = as.matrix(t(pred(NN[[i]], nntrain6[,1:12])), nrow = length(nntrain6[,1]), ncol = 2)
test_result[[i]] = as.matrix(t(pred(NN[[i]], nntest6[,1:12])), nrow = length(nntest6[,1]), ncol= 2)
Mean_test_error[[i]] = mean(test_result[[i]] != nntest6[,c("BK","Not_BK")])
Mean_train_error[[i]] = mean(train_result[[i]] != nntrain6[,c("BK","Not_BK")])
# Haven't figured out how to make tables for all of them yet.
#table_test[i] = table(test_result[[i]][[,1]],  nntrain6$BK)
#table_train[i] = table(train_result[[i]][[,1]],  nntrain6$BK)
}
Mean_test_error[1]
Mean_test_error[[1]]
Mean_test_error[[3]]
Mean_test_error
Mean_train_error
for(i in 2:3){
NN[[i]] = neuralnet(f, data= nntrain6, hidden= i, threshold = 0.1, linear.output = F, lifesign = "full", lifesign.step = 100)
train_result[[i]] = as.matrix(t(pred(NN[[i]], nntrain6[,1:12])), nrow = length(nntrain6[,1]), ncol = 2)
test_result[[i]] = as.matrix(t(pred(NN[[i]], nntest6[,1:12])), nrow = length(nntest6[,1]), ncol= 2)
Mean_test_error[[i]] = mean(test_result[[i]] != nntest6[,c("BK","Not_BK")])
Mean_train_error[[i]] = mean(train_result[[i]] != nntrain6[,c("BK","Not_BK")])
# Haven't figured out how to make tables for all of them yet.
table_test[[i]] = table(test_result[[i]][[,1]],  nntrain6$BK)
table_train[[i]] = table(train_result[[i]][[,1]],  nntrain6$BK)
}
for(i in 2:3){
table_test[[i]] = table(test_result[[i]][[,1]],  nntrain6$BK)
table_train[[i]] = table(train_result[[i]][[,1]],  nntrain6$BK)
}
for(i in 2:3){
table_test[[i]] = table(test_result[[i]][[1]],  nntrain6$BK)
table_train[[i]] = table(train_result[[i]][[1]],  nntrain6$BK)
}
test_result[[1]][[,1]]
test_result[[1]][[,2]]
test_result[[2]][[,2]]
test_result[[2]][[,1]]
test_result[[1]]
test_result[[2]]
test_result[[2]][[1]]
test_result[[2]][[,1]]
test_result[[2]][[1,]]
test_result[[2]][[]]
test_result[[2]][[,]]
test_result[[2]][[1]]
test_result[[2]][[1:length(nntest)]]
test_result[2][1:length(nntest)]
length(nntest)
length(nntest[,1])
test_result[2][1:length(nntest[,1])]
test_result[[1]]
test_result[[2]]
test_result[[2]][1]
test_result[[2]][1:length(nntest[,1])]
for(i in 2:3){
table_test[[i]] = table(test_result[[i]][1:length(nntest[,1])],  nntrain6$BK)
table_train[[i]] = table(train_result[[i]][1:length(nntest[,1])],  nntrain6$BK)
}
for(i in 2:3){
table_test[[i]] = table(test_result[[i]][,1:length(nntest[,1])],  nntrain6$BK)
table_train[[i]] = table(train_result[[i]][,1:length(nntest[,1])],  nntrain6$BK)
}
for(i in 2:3){
table_test[[i]] = table(test_result[[i]][,1:length(nntest[,1])],  nntrain6$BK)
table_train[[i]] = table(train_result[[i]][,1:length(nntrain[,1])],  nntrain6$BK)
}
for(i in 2:3){
table_test[[i]] = table(test_result[[i]][1:length(nntest[,1])],  nntrain6$BK)
table_train[[i]] = table(train_result[[i]][1:length(nntrain[,1])],  nntrain6$BK)
}
length(nntest[,1])
length(nntrain6$BK)
for(i in 2:3){
table_test[[i]] = table(test_result[[i]][1:length(nntest[,1])],  nntest6$BK)
table_train[[i]] = table(train_result[[i]][1:length(nntrain[,1])],  nntrain6$BK)
}
table_test[[1]]
table_test[[2]]
table_test[[3]]
# Input for Minimum Nodes
min = 2
# Input for Max Nodes:
max = 30
for(i in min:max){
NN[[i]] = neuralnet(f, data= nntrain6, hidden= i, threshold = 0.1, linear.output = F, lifesign = "full", lifesign.step = 100)
train_result[[i]] = as.matrix(t(pred(NN[[i]], nntrain6[,1:12])), nrow = length(nntrain6[,1]), ncol = 2)
test_result[[i]] = as.matrix(t(pred(NN[[i]], nntest6[,1:12])), nrow = length(nntest6[,1]), ncol= 2)
Mean_test_error[[i]] = mean(test_result[[i]] != nntest6[,c("BK","Not_BK")])
Mean_train_error[[i]] = mean(train_result[[i]] != nntrain6[,c("BK","Not_BK")])
# Haven't figured out how to make tables for all of them yet.
}
