{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import data\n",
    "decks = pd.read_csv('data.csv')\n",
    "cards = pd.read_json('refs.json')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's only use Ranked Decks.\n",
    "decks = pd.DataFrame(decks)\n",
    "decks['deck_type'] = decks['deck_type'].astype('str') \n",
    "Ranked = pd.DataFrame(decks.loc[decks[\"deck_type\"] == \"Ranked Deck\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sets = list(Ranked['deck_set'].value_counts(sort=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ice Block'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "from pylab import plot, show, savefig, xlim, figure, \\\n",
    "                hold, ylim, legend, boxplot, setp, axes\n",
    "\n",
    "# Use this to find decks with any hint of popularity\n",
    "Ranked2 = Ranked[Ranked[\"rating\"] >= 5]\n",
    "Ranked['year'] = Ranked['date'].astype(str).str[0:4]\n",
    "Ranked2['deck_class'].apply(str) \n",
    "Ranked['year'].apply(str) \n",
    "Ranked['deck_class'].apply(str) \n",
    "#Ranked['year'].apply(str)   \n",
    "# Get the card names here:\n",
    "CARD_DATA = cards[['dbfId',\"name\",\"rarity\",\"playerClass\"]]\n",
    "# get the name of each card:\n",
    "# example:\n",
    "CARD_DATA[CARD_DATA[\"dbfId\"] == 192][\"name\"].value_counts(sort=False).index[0]\n",
    "\n",
    "#years = list(Ranked['year'].value_counts(sort=False).index)\n",
    "#years\n",
    "# We see that we have a very uneven number of deck submissions\n",
    "#print(Ranked['year'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Predictions\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       2014       0.13      0.13      0.13      5185\n",
      "       2015       0.30      0.30      0.30     12282\n",
      "       2016       0.48      0.48      0.48     19583\n",
      "       2017       0.08      0.08      0.08      3423\n",
      "\n",
      "avg / total       0.35      0.35      0.35     40473\n",
      "\n",
      "-0.003457794667190983\n",
      "0.3473784100813215\n",
      "      2014  2015  2016  2017\n",
      "2014   658  1586  2505   436\n",
      "2015  1642  3649  5898  1093\n",
      "2016  2499  5992  9480  1612\n",
      "2017   461  1002  1693   267\n",
      "\n",
      "\n",
      "Naive Bayes Predictions\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       2014       0.16      0.15      0.16      5185\n",
      "       2015       0.53      0.79      0.63     12282\n",
      "       2016       0.83      0.40      0.54     19583\n",
      "       2017       0.21      0.48      0.29      3423\n",
      "\n",
      "avg / total       0.60      0.49      0.50     40473\n",
      "\n",
      "0.2938896287410311\n",
      "0.4978979037119365\n",
      "      2014  2015  2016  2017\n",
      "2014   789  4375    11    10\n",
      "2015  2254  9742   172   114\n",
      "2016  1711  4152  7826  5894\n",
      "2017   119   240  1436  1628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use an ML Algorithm to find out something cool\n",
    "\n",
    "# using the Ranked Dataset, we'll try to classify the upvote levels by card-counts\n",
    "# See if we can classify the set by the card numbers \n",
    "\n",
    "# First try to classify the decks by set number\n",
    "Ranked_MLA = Ranked\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "def NB_Classify(Ranked, log = True):\n",
    "    card_complete = ['card_{}'.format(str(i)) for i in range(30)]\n",
    "    X = Ranked[card_complete][Ranked['year']!= \"2013\"].applymap(str)\n",
    "    Y = Ranked['year'][Ranked['year']!= \"2013\"]\n",
    "    # train / test split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)\n",
    "    # Naive Bayes Classifier\n",
    "    clf = MultinomialNB(alpha=0.1)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    # \n",
    "    clf2 = DummyClassifier()\n",
    "    clf2.fit(X_train, Y_train)\n",
    "    pred2 = clf2.predict(X_test)\n",
    "    \n",
    "    # metrics\n",
    "    Dummy_conf_matrix = pd.DataFrame(\n",
    "        confusion_matrix(Y_test, pred2), \n",
    "        columns=clf.classes_, \n",
    "        index=clf.classes_\n",
    "    )\n",
    "    \n",
    "    conf_matrix = pd.DataFrame(\n",
    "        confusion_matrix(Y_test, pred), \n",
    "        columns=clf.classes_, \n",
    "        index=clf.classes_\n",
    "    )\n",
    "    \n",
    "    if log:\n",
    "        print(\"Random Predictions\")\n",
    "        print (classification_report(Y_test, pred2))\n",
    "        print (cohen_kappa_score(Y_test, pred2))\n",
    "        print (f1_score(Y_test, pred2, average = \"weighted\"))\n",
    "        print (Dummy_conf_matrix)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"Naive Bayes Predictions\")\n",
    "        print (classification_report(Y_test, pred))\n",
    "        print (cohen_kappa_score(Y_test, pred))\n",
    "        print (f1_score(Y_test, pred, average = \"weighted\"))\n",
    "        print (conf_matrix)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "NB_Classify(Ranked, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
